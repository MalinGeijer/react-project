EXPLORATORY DATA ANALYSIS (exploration.ipynb)

1. Visa n√•gra exempelbilder (visuell inspektion)
(Importera bild fr√•n notebook)

Syfte:
det faktiskt √§r siffror
etiketter och bilder matchar
kvaliteten ser rimlig ut
Tolkning:
Bilderna √§r tydliga
Siffrorna √§r hyggligt centrerade
Det finns variation i handstil
De √§r i gr√•skala
28x28px
60000 st tr√§ningsbilder
10000 st testbilder
# x_train = training images
# y_train = training labels, i.e., the correct answers number 0-9
# x_test = test images
# y_test = test labels, i.e., the correct answers number 0-9
# Split the data into a training set and a test set.
# The test set is used to evaluate the model after training. New images that the model has never seen before.
# Preventing overfitting. No guessing.

2. Titta p√• pixelv√§rden (statistisk inspektion av pixelv√§den)

Syfte:
vilket intervall pixlarna ligger i
om normalisering kommer beh√∂vas senare

min ‚âà 0 (svart)
max ‚âà 255 (vitt)
mean ‚âà 30‚Äì40

Tolkning:
Pixlar √§r i originalintervall 0‚Äì255
Normalisering till 0‚Äì1 kommer vara rimligt inf√∂r tr√§ning

3. R√§kna hur m√•nga av varje siffra som finns (klassbalans/F√∂rdelning)
(Importera bild fr√•n notebook)

Syfte:
alla siffror √§r j√§mnt representerade
det finns risk f√∂r obalanserad tr√§ning

Tolkning:
MNIST √§r n√§stan perfekt balanserat
Inga klasser √§r kraftigt underrepresenterade
-------------------------------------------------------------------------------------------------------------------------
DATA CLEANING & PREPROCESSING (02_cleaning.ipynb)


1. Normalisering av pixelv√§rden

Syfte:

F√∂rb√§ttra modellens inl√§rning, alla inputs konkurrerar p√• lika vilkor
G√∂ra ber√§kningarna numeriskt stabila, inl√§rningen g√•r snabbare
Det √§r inte s√• att datorn r√§knar snabbare f√∂r att talen √§r mindre i sig.
Det som g√•r snabbare √§r:

‚úÖ konvergensen (f√§rre tr√§ningssteg beh√∂vs)

‚úÖ optimeringen blir stabil

‚úÖ du slipper extremt sm√• learning rates

S√• tekniskt mer korrekt √§r:

üëâ ‚ÄúNormalisering g√∂r att modellen l√§r sig snabbare och stabilare‚Äù
Skala om v√§rden fr√•n intervallet 0‚Äì255 till 0‚Äì1

Tolkning:

Pixlar l√•g tidigare i originalintervallet 0‚Äì255
Efter normalisering ligger alla v√§rden mellan 0 och 1
Detta g√∂r att modeller tr√§nas snabbare och stabilare

3. Kontroll efter normalisering

Syfte:

Inga v√§rden hamnat utanf√∂r intervallet 0‚Äì1
Ingen data g√•tt f√∂rlorad

Tolkning:

Minsta v√§rde ‚âà 0
H√∂gsta v√§rde ‚âà 1


4. Omformning av bilder till feature-vektorer (flattening)

Syfte:

G√∂ra bilddata kompatibelt med klassiska ML-modeller:
Logistic Regression
Random Forest

Omvandla:

28√ó28 pixlar ‚Üí 784 numeriska features per bild

Tolkning:

Varje bild representeras nu som en rad med 784 v√§rden
Modellerna kan nu tolka varje pixel som en individuell input-variabel

5. Kontroll av nya datadimensioner

Syfte:

S√§kerst√§lla att omformningen har g√•tt r√§tt till
Bekr√§fta att antal observationer √§r of√∂r√§ndrat

Tolkning:

Antalet tr√§nings- och testexempel √§r of√∂r√§ndrat
Endast formen p√• datat har √§ndrats
Datat √§r nu redo f√∂r maskininl√§rning
----------------------------------------------------------------------------------------------------------------------
MODELTEST (logistic_regression.ipynb)

1. Logistic Regression

Logistic Regression √§r linj√§r.
Men‚Ä¶ ‚úÖ den ger ett icke-linj√§rt utdata (sannolikhet) via sigmoiden.

Hur l√§r sig Logistic Regression?

Modellen gissar
Den j√§mf√∂r med facit
Den r√§knar hur fel den hade (log loss)
Den justerar vikterna
Den gissar igen

Detta sker med optimering (L-BFGS i ditt fall).

‚úÖ Vad √§r Logistic Regression?

Trots namnet √§r Logistic Regression egentligen en klassificeringsmodell, inte en regressionsmodell i vanlig mening.

Den svarar p√• fr√•gor som:
‚Äú√Ñr detta en 0 eller inte?‚Äù
‚ÄúVilken siffra 0‚Äì9 √§r detta?‚Äù
‚ÄúKommer en kund s√§ga upp sig eller inte?‚Äù

Allts√•:
Den gissar sannolikhet f√∂r klasser ‚Äì inte ett exakt tal.

Vad √§r Logistic Regression BRA f√∂r?

Den √§r fantastisk n√§r:

‚úÖ 1. Du vill ha en l√§ttf√∂rklarlig modell

Du kan titta p√•:

model.coef_
heatmaps

‚úÖ 2. Du vill ha en stabil baseline

Den anv√§nds ofta som: ‚ÄúF√∂rsta modellen att sl√•‚Äù
Om en avancerad modell inte sl√•r Logistic Regression ‚Üí d√• √§r n√•got fel i datat.

‚úÖ 3. Bra f√∂r:

Ja/Nej
Klass A/B
0‚Äì9 (som MNIST)
Spam/inte spam
Sjuk/frisk

‚úÖ 4. Snabb tr√§ning och l√•g resurs√•tg√•ng

G√•r att tr√§na p√• CPU
Mycket snabb p√• prediction

Perfekt f√∂r:
API
realtidsanv√§ndning
inb√§ddade system

‚ùå Vad √§r Logistic Regression D√ÖLIG p√•?

‚ùå Icke-linj√§ra samband (kr√∂kta m√∂nster)

‚ùå Komplexa bilder (ansikten, f√§rgfoton)

‚ùå N√§r m√∂nstren inte g√•r att separera linj√§rt

‚ùå N√§r relationerna mellan pixlar √§r mer avancerade (d√§r CNN beh√∂vs)

MNIST funkar bra eftersom:

siffrorna √§r centrerade

bakgrunden √§r ren

kontrasten √§r h√∂g

‚úÖ Varf√∂r funkade den s√• bra f√∂r dig (~92‚Äì93 %)?

F√∂r att:

MNIST √§r ett v√§ldigt v√§lstrukturerat dataset

Siffror √§r n√§stan linj√§rt separerbara

Du:

normaliserade korrekt ‚úÖ

flattenade korrekt ‚úÖ

Du gjorde allts√• allt metodiskt r√§tt.

‚úÖ Superkort sammanfattning

Du kan beskriva Logistic Regression s√• h√§r i din rapport:

Logistic Regression √§r en sannolikhetsbaserad klassificeringsmodell som ber√§knar sannolikheten
f√∂r varje klass genom en viktad summering av indata f√∂ljt av en softmax-funktion.
Modellen tr√§nas genom att minimera log-f√∂rlust och justera vikterna iterativt.
Den l√§mpar sig v√§l som baseline-modell och fungerar effektivt f√∂r linj√§rt separerbara problem s√•som MNIST.

Total accuracy on test set: 0.9265

              precision    recall  f1-score   support

           0       0.95      0.98      0.97       980
           1       0.96      0.98      0.97      1135
           2       0.93      0.90      0.92      1032
           3       0.90      0.91      0.91      1010
           4       0.94      0.94      0.94       982
           5       0.90      0.87      0.88       892
           6       0.94      0.95      0.95       958
           7       0.94      0.93      0.93      1028
           8       0.88      0.88      0.88       974
           9       0.91      0.92      0.92      1009

    accuracy                           0.93     10000
   macro avg       0.93      0.93      0.93     10000
weighted avg       0.93      0.93      0.93     10000

precision per siffra

recall per siffra

f1-score per siffra

weighted / macro averages

Detta √§r guld i rapporten eftersom du kan resonera om:

vilka siffror modellen har sv√•rast f√∂r

vilka siffror √§r enklast

om datat √§r balanserat (MNIST √§r n√§stan perfekt)

Confusion matrix
diagonalen = r√§tt gissningar

allt utanf√∂r diagonalen = fel gissningar

Detta √§r den b√§sta visuella analysen av modellen.
------------------------------------------------------------------------------------------------------------

2. Random Forrest

Icke linj√§r -> massor av beslutstr√§d ‚Üí kan l√§ra sig kr√∂kta, komplexa m√∂nster
Random Forest Accuracy: 0.9704
Random Forest visar h√∂gst pixelviktighet i bildens centrala delar d√§r siffrorna √§r lokaliserade,
medan bakgrundspixlar i h√∂rnen bidrar mycket lite till klassificeringen.
